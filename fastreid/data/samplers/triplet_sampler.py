# encoding: utf-8
"""
@author:  liaoxingyu
@contact: liaoxingyu2@jd.com
"""

import copy
import itertools
from collections import defaultdict
from typing import Optional

import numpy as np
from torch.utils.data.sampler import Sampler

from fastreid.utils import comm
from . import SAMPLER_REGISTRY


def no_index(a, b):
    assert isinstance(a, list)
    return [i for i, j in enumerate(a) if j != b]


def reorder_index(batch_indices, world_size):
    r"""Reorder indices of samples to align with DataParallel training.
    In this order, each process will contain all images for one ID, triplet loss
    can be computed within each process, and BatchNorm will get a stable result.
    Args:
        batch_indices: A batched indices generated by sampler
        world_size: number of process
    Returns:

    """
    mini_batchsize = len(batch_indices) // world_size
    reorder_indices = []
    for i in range(0, mini_batchsize):
        for j in range(0, world_size):
            reorder_indices.append(batch_indices[i + j * mini_batchsize])
    return reorder_indices


@SAMPLER_REGISTRY.register()
class BalancedIdentitySampler(Sampler):
    def __init__(self, data_source: list, batch_size: int, num_instances: int, seed: Optional[int] = None, **kwargs):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = batch_size // self.num_instances

        self.index_pid = dict()
        self.pid_cam = defaultdict(list)
        self.pid_index = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info[1]
            camid = info[2]
            if pid == -1: continue  # ignore unused instances
            self.index_pid[index] = pid
            self.pid_cam[pid].append(camid)
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            # Shuffle identity list
            identities = np.random.permutation(self.num_identities)

            # If remaining identities cannot be enough for a batch,
            # just drop the remaining parts
            drop_indices = self.num_identities % (self.num_pids_per_batch * self._world_size)
            if drop_indices: identities = identities[:-drop_indices]

            batch_indices = []
            for kid in identities:
                i = np.random.choice(self.pid_index[self.pids[kid]])
                _, i_pid, i_cam = self.data_source[i]
                batch_indices.append(i)
                pid_i = self.index_pid[i]
                cams = self.pid_cam[pid_i]
                index = self.pid_index[pid_i]
                select_cams = no_index(cams, i_cam)

                if select_cams:
                    if len(select_cams) >= self.num_instances:
                        cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=False)
                    else:
                        cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=True)
                    for kk in cam_indexes:
                        batch_indices.append(index[kk])
                else:
                    select_indexes = no_index(index, i)
                    if not select_indexes:
                        # Only one image for this identity
                        ind_indexes = [0] * (self.num_instances - 1)
                    elif len(select_indexes) >= self.num_instances:
                        ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=False)
                    else:
                        ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=True)

                    for kk in ind_indexes:
                        batch_indices.append(index[kk])

                if len(batch_indices) == self.batch_size:
                    yield from reorder_index(batch_indices, self._world_size)
                    batch_indices = []


@SAMPLER_REGISTRY.register()
class NaiveIdentitySampler(Sampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source: list, batch_size: int, num_instances: int, seed: Optional[int] = None, **kwargs):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = batch_size // self.num_instances

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = batch_size

        self.pid_index = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info[1]
            if pid == -1: continue  # ignore unused instances
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            avl_pids = copy.deepcopy(self.pids)
            batch_idxs_dict = {}

            batch_indices = []
            while len(avl_pids) >= self.num_pids_per_batch:
                selected_pids = np.random.choice(avl_pids, self.num_pids_per_batch, replace=False).tolist()
                for pid in selected_pids:
                    # Register pid in batch_idxs_dict if not
                    if pid not in batch_idxs_dict:
                        idxs = copy.deepcopy(self.pid_index[pid])
                        if len(idxs) < self.num_instances:
                            idxs = np.random.choice(idxs, size=self.num_instances, replace=True).tolist()
                        np.random.shuffle(idxs)
                        batch_idxs_dict[pid] = idxs

                    avl_idxs = batch_idxs_dict[pid]
                    for _ in range(self.num_instances):
                        batch_indices.append(avl_idxs.pop(0))

                    if len(avl_idxs) < self.num_instances: avl_pids.remove(pid)

                if len(batch_indices) == self.batch_size:
                    yield from reorder_index(batch_indices, self._world_size)
                batch_indices = []


@SAMPLER_REGISTRY.register()
class RandomMultipleGallerySampler(Sampler):
    def __init__(self, data_source: list, batch_size: int, num_instances: int = 4, seed: Optional[int] = None, **kwargs):
        self.data_source = data_source
        self.num_instances = num_instances
        self.batch_size = batch_size

        self.index_pid = defaultdict(int)
        self.pid_cam = defaultdict(list)
        self.pid_index = defaultdict(list)

        for index, (_, pid, cam) in enumerate(data_source):
            if pid == -1: continue  # ignore unused instances
            self.index_pid[index] = pid
            self.pid_cam[pid].append(cam)
            self.pid_index[pid].append(index)

        self.pids = list(self.pid_index.keys())
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)
        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            indices = np.random.permutation(self.num_identities).tolist()
            ret = []

            for kid in indices:
                i = np.random.choice(self.pid_index[self.pids[kid]])
                _, i_pid, i_cam = self.data_source[i]
                ret.append(i)
                pid_i = self.index_pid[i]
                cams = self.pid_cam[pid_i]
                index = self.pid_index[pid_i]
                select_cams = no_index(cams, i_cam)

                if select_cams:
                    if len(select_cams) >= self.num_instances:
                        cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=False)
                    else:
                        cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=True)
                    for kk in cam_indexes:
                        ret.append(index[kk])
                else:
                    select_indexes = no_index(index, i)
                    if (not select_indexes):
                        continue
                    if len(select_indexes) >= self.num_instances:
                        ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=False)
                    else:
                        ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=True)

                    for kk in ind_indexes:
                        ret.append(index[kk])

                if len(ret) == self.batch_size:
                    yield from reorder_index(ret, self._world_size)
                    ret = []
            # yield from ret

@SAMPLER_REGISTRY.register()
class MultiDomainSampler(Sampler):
    """
    Sampler for multiple datasets, when num_domains = M, and batch_size = N, int(N/M) images for each domain, used for domain-specific BN.
    """
    def __init__(self, data_source: list, batch_size: int, num_instances: int = 4, seed: Optional[int] = None, train_set: list=None, **kwargs):
        assert train_set is not None, "datasets is required for MultiDomainSampler"
        self.data_source = data_source
        self.num_instances = num_instances
        self.batch_size = batch_size

        self.num_domains = len(train_set.datasets)
        self.img_size_list = list(itertools.accumulate([len(d) for d in train_set.datasets]))

        self.index_pid = [defaultdict(int) for _ in range(self.num_domains)]
        self.pid_cam = [defaultdict(list) for _ in range(self.num_domains)]
        self.pid_index = [defaultdict(list) for _ in range(self.num_domains)]

        for index, (_, pid, cam) in enumerate(data_source):
            if pid == -1: continue  # ignore unused instances
            domain_index = next(x[0] for x in enumerate(self.img_size_list) if x[1] > index)
            self.index_pid[domain_index][index] = pid
            self.pid_cam[domain_index][pid].append(cam)
            self.pid_index[domain_index][pid].append(index)

        self.pids = [list(self.pid_index[ind].keys()) for ind in range(len(self.pid_index))]
        self.num_identities = [len(pids) for pids in self.pids]

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)
        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            ret_combined = []
            indices = [np.random.permutation(self.pids[did]).tolist() for did in range(self.num_domains)]
            for did in range(self.num_domains):
                ret = []
                for kid in indices[did]:
                    i = np.random.choice(self.pid_index[did][kid])
                    _, i_pid, i_cam = self.data_source[i]
                    ret.append(i)
                    pid_i = self.index_pid[did][i]
                    cams = self.pid_cam[did][pid_i]
                    index = self.pid_index[did][pid_i]
                    select_cams = no_index(cams, i_cam)

                    if select_cams:
                        if len(select_cams) >= self.num_instances:
                            cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=False)
                        else:
                            cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=True)
                        for kk in cam_indexes:
                            ret.append(index[kk])
                    else:
                        select_indexes = no_index(index, i)
                        if (not select_indexes):
                            continue  # exist a pid only contain a single image.
                        if len(select_indexes) >= self.num_instances:
                            ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=False)
                        else:
                            ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=True)

                        for kk in ind_indexes:
                            ret.append(index[kk])
                    if len(ret) >= (self.batch_size // self.num_domains):
                        ret_combined.extend(reorder_index(ret[:self.batch_size // self.num_domains], self._world_size))
                        break
            yield from ret_combined

@SAMPLER_REGISTRY.register()
class ProxyBalancedSampler(Sampler):
    def __init__(self, data_source: list, batch_size: int, num_instances: int = 4, seed: Optional[int] = None, **kwargs):
        self.data_source = data_source
        self.num_instances = num_instances
        self.batch_size = batch_size

        self.get_labels()
        self.pids = list(self.pid_index.keys())
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)
        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def get_labels(self):
        all_label = []
        all_cams = []
        for _, pid, cam in self.data_source:
            all_label.append(pid)
            all_cams.append(cam)
        all_label = np.array(all_label)
        all_cams = np.array(all_cams)
        pure_label = all_label[all_label >= 0]
        pure_cams = all_cams[all_label >= 0]
        accum_labels = np.zeros(len(pure_label))
        prev_id_count = 0
        id_count_each_cam = []
        for cc in np.unique(pure_cams):
            percam_labels = pure_label[pure_cams == cc]
            unique_id = np.unique(percam_labels)
            id_count_each_cam.append(len(unique_id))
            id_dict = {ID: i for i, ID in enumerate(unique_id.tolist())}
            for i in range(len(percam_labels)):
                percam_labels[i] = id_dict[percam_labels[i]]
            accum_labels[pure_cams == cc] = percam_labels + prev_id_count
            prev_id_count += len(unique_id)
        new_accum_labels = -1 * np.ones(all_label.shape, all_label.dtype)
        new_accum_labels[all_label>=0] = accum_labels

        self.pid_index = defaultdict(list)
        for index, accum_pid in enumerate(new_accum_labels):
            if accum_pid == -1: continue
            self.pid_index[accum_pid].append(index)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            indices = np.random.permutation(self.num_identities).tolist()
            ret = []

            for kid in indices:
                select_indexes = self.pid_index[kid]
                if len(select_indexes) >= self.num_instances:
                    indexes = np.random.choice(select_indexes, size=self.num_instances, replace=False)
                else:
                    indexes = np.random.choice(select_indexes, size=self.num_instances, replace=True)
                ret.extend(indexes)

                if len(ret) == self.batch_size:
                    yield from reorder_index(ret, self._world_size)
                    ret = []
            # yield from ret

@SAMPLER_REGISTRY.register()
class ProxySampler(Sampler):
    def __init__(self, data_source: list, batch_size: int, num_instances: int = 4, seed: Optional[int] = None, **kwargs):
        self.data_source = data_source
        self.num_instances = num_instances
        self.batch_size = batch_size

        self.pid_cam_index = defaultdict(list)

        for index, (_, pid, cam) in enumerate(data_source):
            if pid == -1: continue  # ignore unused instances
            self.pid_cam_index[(pid, cam)].append(index)

        self.pids_cams = [k for k in self.pid_cam_index.keys()]
        self.num_identities = len(self.pids_cams)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)
        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            indices = np.random.permutation(self.num_identities).tolist()
            ret = []

            for kid in indices:
                select_indexes = self.pid_cam_index[self.pids_cams[kid]]
                if len(select_indexes) >= self.num_instances:
                    indexes = np.random.choice(select_indexes, size=self.num_instances, replace=False)
                else:
                    indexes = np.random.choice(select_indexes, size=self.num_instances, replace=True)
                ret.extend(indexes)

                if len(ret) == self.batch_size:
                    yield from reorder_index(ret, self._world_size)
                    ret = []
            # yield from ret
